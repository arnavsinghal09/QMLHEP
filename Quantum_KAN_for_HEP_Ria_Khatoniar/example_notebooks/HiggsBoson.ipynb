{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9I-Ta9l3YgT9",
        "outputId": "1a2961da-4475-44a2-97c5-ba8fab1f87ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pennylane\n",
            "  Downloading pennylane-0.42.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from pennylane) (1.15.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from pennylane) (3.5)\n",
            "Collecting rustworkx>=0.14.0 (from pennylane)\n",
            "  Downloading rustworkx-0.16.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: autograd in /usr/local/lib/python3.11/dist-packages (from pennylane) (1.8.0)\n",
            "Collecting appdirs (from pennylane)\n",
            "  Downloading appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting autoray>=0.6.11 (from pennylane)\n",
            "  Downloading autoray-0.7.2-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.11/dist-packages (from pennylane) (5.5.2)\n",
            "Collecting pennylane-lightning>=0.42 (from pennylane)\n",
            "  Downloading pennylane_lightning-0.42.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (11 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from pennylane) (2.32.3)\n",
            "Requirement already satisfied: tomlkit in /usr/local/lib/python3.11/dist-packages (from pennylane) (0.13.3)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.11/dist-packages (from pennylane) (4.14.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from pennylane) (25.0)\n",
            "Collecting diastatic-malt (from pennylane)\n",
            "  Downloading diastatic_malt-2.15.2-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from pennylane) (2.0.2)\n",
            "Collecting scipy-openblas32>=0.3.26 (from pennylane-lightning>=0.42->pennylane)\n",
            "  Downloading scipy_openblas32-0.3.30.0.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.1/57.1 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: astunparse in /usr/local/lib/python3.11/dist-packages (from diastatic-malt->pennylane) (1.6.3)\n",
            "Requirement already satisfied: gast in /usr/local/lib/python3.11/dist-packages (from diastatic-malt->pennylane) (0.6.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.11/dist-packages (from diastatic-malt->pennylane) (3.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->pennylane) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->pennylane) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->pennylane) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->pennylane) (2025.7.14)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse->diastatic-malt->pennylane) (0.45.1)\n",
            "Requirement already satisfied: six<2.0,>=1.6.1 in /usr/local/lib/python3.11/dist-packages (from astunparse->diastatic-malt->pennylane) (1.17.0)\n",
            "Downloading pennylane-0.42.0-py3-none-any.whl (4.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m74.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading autoray-0.7.2-py3-none-any.whl (930 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m930.8/930.8 kB\u001b[0m \u001b[31m54.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pennylane_lightning-0.42.0-cp311-cp311-manylinux_2_28_x86_64.whl (2.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m71.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rustworkx-0.16.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m63.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
            "Downloading diastatic_malt-2.15.2-py3-none-any.whl (167 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.9/167.9 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy_openblas32-0.3.30.0.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m58.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: appdirs, scipy-openblas32, rustworkx, autoray, diastatic-malt, pennylane-lightning, pennylane\n",
            "Successfully installed appdirs-1.4.4 autoray-0.7.2 diastatic-malt-2.15.2 pennylane-0.42.0 pennylane-lightning-0.42.0 rustworkx-0.16.0 scipy-openblas32-0.3.30.0.1\n"
          ]
        }
      ],
      "source": [
        "!pip install pennylane"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xEnx-NixESPv",
        "outputId": "01a7398e-e7b9-468f-c1b3-99b78a061823"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w2qJ0C5AE345",
        "outputId": "91c2a0c8-da6b-4f58-d933-b0854ea62a9c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of loaded dataset: (10000, 29)\n"
          ]
        }
      ],
      "source": [
        "file_path = \"/content/drive/MyDrive/HIGGS.csv\"\n",
        "\n",
        "df = pd.read_csv(file_path, nrows=10000, header=None)\n",
        "print(\"Shape of loaded dataset:\", df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tZ4llsUhFDQw",
        "outputId": "1ef2ca0e-1d52-48b3-f120-49768219553a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Feature matrix X shape: (10000, 28)\n",
            "Label vector y shape: (10000,)\n"
          ]
        }
      ],
      "source": [
        "y = df.iloc[:, 0].values\n",
        "X = df.iloc[:, 1:].values\n",
        "print(\"Feature matrix X shape:\", X.shape)\n",
        "print(\"Label vector y shape:\", y.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZkkEZq0LFIWx",
        "outputId": "3e89f4e5-e13e-4786-c238-38f35d3ba2f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    0         1         2         3         4         5         6         7   \\\n",
            "0  1.0  0.869293 -0.635082  0.225690  0.327470 -0.689993  0.754202 -0.248573   \n",
            "1  1.0  0.907542  0.329147  0.359412  1.497970 -0.313010  1.095531 -0.557525   \n",
            "2  1.0  0.798835  1.470639 -1.635975  0.453773  0.425629  1.104875  1.282322   \n",
            "3  0.0  1.344385 -0.876626  0.935913  1.992050  0.882454  1.786066 -1.646778   \n",
            "4  1.0  1.105009  0.321356  1.522401  0.882808 -1.205349  0.681466 -1.070464   \n",
            "\n",
            "         8         9   ...        18        19        20        21        22  \\\n",
            "0 -1.092064  0.000000  ...  0.657930 -0.010455 -0.045767  3.101961  1.353760   \n",
            "1 -1.588230  2.173076  ...  0.398701 -1.138930 -0.000819  0.000000  0.302220   \n",
            "2  1.381664  0.000000  ...  1.256955  1.128848  0.900461  0.000000  0.909753   \n",
            "3 -0.942383  0.000000  ...  0.745313 -0.678379 -1.360356  0.000000  0.946652   \n",
            "4 -0.921871  0.000000  ...  0.479999 -0.373566  0.113041  0.000000  0.755856   \n",
            "\n",
            "         23        24        25        26        27  \n",
            "0  0.979563  0.978076  0.920005  0.721657  0.988751  \n",
            "1  0.833048  0.985700  0.978098  0.779732  0.992356  \n",
            "2  1.108330  0.985692  0.951331  0.803252  0.865924  \n",
            "3  1.028704  0.998656  0.728281  0.869200  1.026736  \n",
            "4  1.361057  0.986610  0.838085  1.133295  0.872245  \n",
            "\n",
            "[5 rows x 28 columns]\n"
          ]
        }
      ],
      "source": [
        "print(df.iloc[:5, :28])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zvcMHi_nGi0J",
        "outputId": "5389ef7d-f896-4b0e-9f62-7729030f55bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pennylane\n",
            "  Downloading pennylane-0.42.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from pennylane) (1.15.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from pennylane) (3.5)\n",
            "Collecting rustworkx>=0.14.0 (from pennylane)\n",
            "  Downloading rustworkx-0.16.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: autograd in /usr/local/lib/python3.11/dist-packages (from pennylane) (1.8.0)\n",
            "Collecting appdirs (from pennylane)\n",
            "  Downloading appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting autoray>=0.6.11 (from pennylane)\n",
            "  Downloading autoray-0.7.2-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.11/dist-packages (from pennylane) (5.5.2)\n",
            "Collecting pennylane-lightning>=0.42 (from pennylane)\n",
            "  Downloading pennylane_lightning-0.42.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (11 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from pennylane) (2.32.3)\n",
            "Requirement already satisfied: tomlkit in /usr/local/lib/python3.11/dist-packages (from pennylane) (0.13.3)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.11/dist-packages (from pennylane) (4.14.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from pennylane) (25.0)\n",
            "Collecting diastatic-malt (from pennylane)\n",
            "  Downloading diastatic_malt-2.15.2-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from pennylane) (2.0.2)\n",
            "Collecting scipy-openblas32>=0.3.26 (from pennylane-lightning>=0.42->pennylane)\n",
            "  Downloading scipy_openblas32-0.3.30.0.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.1/57.1 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: astunparse in /usr/local/lib/python3.11/dist-packages (from diastatic-malt->pennylane) (1.6.3)\n",
            "Requirement already satisfied: gast in /usr/local/lib/python3.11/dist-packages (from diastatic-malt->pennylane) (0.6.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.11/dist-packages (from diastatic-malt->pennylane) (3.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->pennylane) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->pennylane) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->pennylane) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->pennylane) (2025.7.14)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse->diastatic-malt->pennylane) (0.45.1)\n",
            "Requirement already satisfied: six<2.0,>=1.6.1 in /usr/local/lib/python3.11/dist-packages (from astunparse->diastatic-malt->pennylane) (1.17.0)\n",
            "Downloading pennylane-0.42.0-py3-none-any.whl (4.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading autoray-0.7.2-py3-none-any.whl (930 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m930.8/930.8 kB\u001b[0m \u001b[31m55.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pennylane_lightning-0.42.0-cp311-cp311-manylinux_2_28_x86_64.whl (2.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m73.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rustworkx-0.16.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m71.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
            "Downloading diastatic_malt-2.15.2-py3-none-any.whl (167 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.9/167.9 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy_openblas32-0.3.30.0.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m75.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: appdirs, scipy-openblas32, rustworkx, autoray, diastatic-malt, pennylane-lightning, pennylane\n",
            "Successfully installed appdirs-1.4.4 autoray-0.7.2 diastatic-malt-2.15.2 pennylane-0.42.0 pennylane-lightning-0.42.0 rustworkx-0.16.0 scipy-openblas32-0.3.30.0.1\n"
          ]
        }
      ],
      "source": [
        "!pip install pennylane"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cwHOlWyTiiXy",
        "outputId": "123b8fc7-2a3e-485c-863f-b38a6eed233e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[SineKAN HIGGS] Epoch 01 | Loss: 0.9329 | Test Acc: 0.4800\n",
            "[SineKAN HIGGS] Epoch 02 | Loss: 1.0716 | Test Acc: 0.4750\n",
            "[SineKAN HIGGS] Epoch 03 | Loss: 0.6972 | Test Acc: 0.4950\n",
            "[SineKAN HIGGS] Epoch 04 | Loss: 0.6962 | Test Acc: 0.5150\n",
            "[SineKAN HIGGS] Epoch 05 | Loss: 0.6999 | Test Acc: 0.5100\n",
            "[SineKAN HIGGS] Epoch 06 | Loss: 0.7005 | Test Acc: 0.5100\n",
            "[SineKAN HIGGS] Epoch 07 | Loss: 0.6927 | Test Acc: 0.5350\n",
            "[SineKAN HIGGS] Epoch 08 | Loss: 0.6847 | Test Acc: 0.5550\n",
            "[SineKAN HIGGS] Epoch 09 | Loss: 0.6764 | Test Acc: 0.5850\n",
            "[SineKAN HIGGS] Epoch 10 | Loss: 0.6694 | Test Acc: 0.5950\n",
            "[SineKAN HIGGS] Epoch 11 | Loss: 0.6646 | Test Acc: 0.5800\n",
            "[SineKAN HIGGS] Epoch 12 | Loss: 0.6616 | Test Acc: 0.5600\n",
            "[SineKAN HIGGS] Epoch 13 | Loss: 0.6593 | Test Acc: 0.5450\n",
            "[SineKAN HIGGS] Epoch 14 | Loss: 0.6572 | Test Acc: 0.5550\n",
            "[SineKAN HIGGS] Epoch 15 | Loss: 0.6543 | Test Acc: 0.5750\n",
            "[SineKAN HIGGS] Epoch 16 | Loss: 0.6498 | Test Acc: 0.5550\n",
            "[SineKAN HIGGS] Epoch 17 | Loss: 0.6438 | Test Acc: 0.5500\n",
            "[SineKAN HIGGS] Epoch 18 | Loss: 0.6374 | Test Acc: 0.5600\n",
            "[SineKAN HIGGS] Epoch 19 | Loss: 0.6315 | Test Acc: 0.5750\n",
            "[SineKAN HIGGS] Epoch 20 | Loss: 0.6259 | Test Acc: 0.5750\n",
            "[SineKAN HIGGS] Epoch 21 | Loss: 0.6197 | Test Acc: 0.5800\n",
            "[SineKAN HIGGS] Epoch 22 | Loss: 0.6125 | Test Acc: 0.5900\n",
            "[SineKAN HIGGS] Epoch 23 | Loss: 0.6049 | Test Acc: 0.5750\n",
            "[SineKAN HIGGS] Epoch 24 | Loss: 0.5969 | Test Acc: 0.5750\n",
            "[SineKAN HIGGS] Epoch 25 | Loss: 0.5880 | Test Acc: 0.6050\n",
            "[SineKAN HIGGS] Epoch 26 | Loss: 0.5783 | Test Acc: 0.6000\n",
            "[SineKAN HIGGS] Epoch 27 | Loss: 0.5678 | Test Acc: 0.6100\n",
            "[SineKAN HIGGS] Epoch 28 | Loss: 0.5570 | Test Acc: 0.6300\n",
            "[SineKAN HIGGS] Epoch 29 | Loss: 0.5460 | Test Acc: 0.6450\n",
            "[SineKAN HIGGS] Epoch 30 | Loss: 0.5349 | Test Acc: 0.6350\n",
            "[SineKAN HIGGS] Epoch 31 | Loss: 0.5232 | Test Acc: 0.6550\n",
            "[SineKAN HIGGS] Epoch 32 | Loss: 0.5102 | Test Acc: 0.6500\n",
            "[SineKAN HIGGS] Epoch 33 | Loss: 0.4958 | Test Acc: 0.6250\n",
            "[SineKAN HIGGS] Epoch 34 | Loss: 0.4819 | Test Acc: 0.6550\n",
            "[SineKAN HIGGS] Epoch 35 | Loss: 0.4676 | Test Acc: 0.6450\n",
            "[SineKAN HIGGS] Epoch 36 | Loss: 0.4520 | Test Acc: 0.6450\n",
            "[SineKAN HIGGS] Epoch 37 | Loss: 0.4362 | Test Acc: 0.6550\n",
            "[SineKAN HIGGS] Epoch 38 | Loss: 0.4203 | Test Acc: 0.6200\n",
            "[SineKAN HIGGS] Epoch 39 | Loss: 0.4034 | Test Acc: 0.6200\n",
            "[SineKAN HIGGS] Epoch 40 | Loss: 0.3865 | Test Acc: 0.5900\n",
            "[SineKAN HIGGS] Epoch 41 | Loss: 0.3714 | Test Acc: 0.6150\n",
            "[SineKAN HIGGS] Epoch 42 | Loss: 0.3584 | Test Acc: 0.5900\n",
            "[SineKAN HIGGS] Epoch 43 | Loss: 0.3431 | Test Acc: 0.6150\n",
            "[SineKAN HIGGS] Epoch 44 | Loss: 0.3255 | Test Acc: 0.6050\n",
            "[SineKAN HIGGS] Epoch 45 | Loss: 0.3139 | Test Acc: 0.6150\n",
            "[SineKAN HIGGS] Epoch 46 | Loss: 0.2978 | Test Acc: 0.6200\n",
            "[SineKAN HIGGS] Epoch 47 | Loss: 0.2842 | Test Acc: 0.6050\n",
            "[SineKAN HIGGS] Epoch 48 | Loss: 0.2698 | Test Acc: 0.6200\n",
            "[SineKAN HIGGS] Epoch 49 | Loss: 0.2569 | Test Acc: 0.5850\n",
            "[SineKAN HIGGS] Epoch 50 | Loss: 0.2444 | Test Acc: 0.6000\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "import math\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "class SineKANLayer(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, grid_size=5):\n",
        "        super().__init__()\n",
        "        self.grid_size = grid_size\n",
        "        self.A, self.K, self.C = 0.9724, 0.9884, 0.9994\n",
        "        self.grid_norm = (torch.arange(grid_size, dtype=torch.float32) + 1).reshape(1, 1, grid_size)\n",
        "        amp = torch.empty(output_dim, input_dim, 1).normal_(0, .4) / output_dim / self.grid_norm\n",
        "        self.amplitudes = nn.Parameter(amp.float())\n",
        "        self.freq = nn.Parameter(torch.arange(1, grid_size + 1).float().reshape(1, 1, 1, grid_size))\n",
        "        grid_phase = torch.arange(1, grid_size + 1, dtype=torch.float32).reshape(1, 1, 1, grid_size) / (grid_size + 1)\n",
        "        input_phase = torch.linspace(0, math.pi, input_dim, dtype=torch.float32).reshape(1, 1, input_dim, 1)\n",
        "        phase = grid_phase + input_phase\n",
        "        for _ in range(1, grid_size):\n",
        "            phase = (self.A * grid_size ** (-self.K) + self.C) * phase\n",
        "        self.register_buffer(\"phase\", phase)\n",
        "        self.bias = nn.Parameter(torch.ones(1, output_dim).float() / output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.to(self.amplitudes.dtype)\n",
        "        x = x.view(x.shape[0], -1).unsqueeze(-1).unsqueeze(1)\n",
        "        s = torch.sin(x * self.freq + self.phase)\n",
        "        y = torch.einsum('bijd,ojd->bo', s, self.amplitudes)\n",
        "        return y + self.bias\n",
        "\n",
        "class SineKAN(nn.Module):\n",
        "    def __init__(self, input_dim, hidden=64, output_dim=2):\n",
        "        super().__init__()\n",
        "        self.l1 = SineKANLayer(input_dim, hidden)\n",
        "        self.l2 = SineKANLayer(hidden, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.l2(self.l1(x))\n",
        "\n",
        "def load_higgs_data(path, Nsamples=1000):\n",
        "    df = pd.read_csv(path, header=None)\n",
        "    df = df.sample(frac=1.0, random_state=42)\n",
        "    X = df.iloc[:Nsamples, 1:].values\n",
        "    y = df.iloc[:Nsamples, 0].values\n",
        "    X_min = X.min(axis=1, keepdims=True)\n",
        "    X_max = X.max(axis=1, keepdims=True)\n",
        "    X_scaled = 2 * (X - X_min) / (X_max - X_min + 1e-8) - 1\n",
        "    return train_test_split(\n",
        "        torch.tensor(X_scaled, dtype=torch.float32),\n",
        "        torch.tensor(y, dtype=torch.long),\n",
        "        test_size=0.2,\n",
        "        stratify=y,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "def train_sinekan_higgs(file_path):\n",
        "    X_train, X_test, y_train, y_test = load_higgs_data(file_path, Nsamples=1000)\n",
        "    input_dim = X_train.shape[1]\n",
        "    model = SineKAN(input_dim=input_dim, hidden=14, output_dim=2)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    X_train = X_train.to(device)\n",
        "    X_test = X_test.to(device)\n",
        "    y_train = y_train.to(device)\n",
        "    y_test = y_test.to(device)\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=1e-2)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    for epoch in range(50):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        preds = model(X_train)\n",
        "        loss = criterion(preds, y_train)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            acc = (model(X_test).argmax(dim=1) == y_test).float().mean().item()\n",
        "        print(f\"[SineKAN HIGGS] Epoch {epoch+1:02d} | Loss: {loss.item():.4f} | Test Acc: {acc:.4f}\")\n",
        "\n",
        "train_sinekan_higgs(file_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pORwMF5nu-H_",
        "outputId": "42bf0da7-592a-4b48-e79f-b12a4dab884d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[SineKAN HIGGS] Epoch 01 | Loss: 1.4266 | Test Acc: 0.5250\n",
            "[SineKAN HIGGS] Epoch 02 | Loss: 0.7333 | Test Acc: 0.5000\n",
            "[SineKAN HIGGS] Epoch 03 | Loss: 0.7044 | Test Acc: 0.5300\n",
            "[SineKAN HIGGS] Epoch 04 | Loss: 0.6920 | Test Acc: 0.5350\n",
            "[SineKAN HIGGS] Epoch 05 | Loss: 0.6864 | Test Acc: 0.5550\n",
            "[SineKAN HIGGS] Epoch 06 | Loss: 0.6832 | Test Acc: 0.5500\n",
            "[SineKAN HIGGS] Epoch 07 | Loss: 0.6795 | Test Acc: 0.5350\n",
            "[SineKAN HIGGS] Epoch 08 | Loss: 0.6779 | Test Acc: 0.5450\n",
            "[SineKAN HIGGS] Epoch 09 | Loss: 0.6759 | Test Acc: 0.5450\n",
            "[SineKAN HIGGS] Epoch 10 | Loss: 0.6740 | Test Acc: 0.5650\n",
            "[SineKAN HIGGS] Epoch 11 | Loss: 0.6708 | Test Acc: 0.5650\n",
            "[SineKAN HIGGS] Epoch 12 | Loss: 0.6632 | Test Acc: 0.5750\n",
            "[SineKAN HIGGS] Epoch 13 | Loss: 0.6551 | Test Acc: 0.5750\n",
            "[SineKAN HIGGS] Epoch 14 | Loss: 0.6479 | Test Acc: 0.5900\n",
            "[SineKAN HIGGS] Epoch 15 | Loss: 0.6393 | Test Acc: 0.5950\n",
            "[SineKAN HIGGS] Epoch 16 | Loss: 0.6340 | Test Acc: 0.6050\n",
            "[SineKAN HIGGS] Epoch 17 | Loss: 0.6260 | Test Acc: 0.5900\n",
            "[SineKAN HIGGS] Epoch 18 | Loss: 0.6170 | Test Acc: 0.5900\n",
            "[SineKAN HIGGS] Epoch 19 | Loss: 0.6055 | Test Acc: 0.6100\n",
            "[SineKAN HIGGS] Epoch 20 | Loss: 0.5944 | Test Acc: 0.6050\n",
            "[SineKAN HIGGS] Epoch 21 | Loss: 0.5846 | Test Acc: 0.6300\n",
            "[SineKAN HIGGS] Epoch 22 | Loss: 0.5734 | Test Acc: 0.6050\n",
            "[SineKAN HIGGS] Epoch 23 | Loss: 0.5621 | Test Acc: 0.6250\n",
            "[SineKAN HIGGS] Epoch 24 | Loss: 0.5507 | Test Acc: 0.6500\n",
            "[SineKAN HIGGS] Epoch 25 | Loss: 0.5374 | Test Acc: 0.6350\n",
            "[SineKAN HIGGS] Epoch 26 | Loss: 0.5243 | Test Acc: 0.6300\n",
            "[SineKAN HIGGS] Epoch 27 | Loss: 0.5116 | Test Acc: 0.6350\n",
            "[SineKAN HIGGS] Epoch 28 | Loss: 0.4961 | Test Acc: 0.6200\n",
            "[SineKAN HIGGS] Epoch 29 | Loss: 0.4844 | Test Acc: 0.5900\n",
            "[SineKAN HIGGS] Epoch 30 | Loss: 0.4704 | Test Acc: 0.5850\n",
            "[SineKAN HIGGS] Epoch 31 | Loss: 0.4564 | Test Acc: 0.5750\n",
            "[SineKAN HIGGS] Epoch 32 | Loss: 0.4426 | Test Acc: 0.5850\n",
            "[SineKAN HIGGS] Epoch 33 | Loss: 0.4284 | Test Acc: 0.6000\n",
            "[SineKAN HIGGS] Epoch 34 | Loss: 0.4131 | Test Acc: 0.5950\n",
            "[SineKAN HIGGS] Epoch 35 | Loss: 0.3966 | Test Acc: 0.6000\n",
            "[SineKAN HIGGS] Epoch 36 | Loss: 0.3819 | Test Acc: 0.5900\n",
            "[SineKAN HIGGS] Epoch 37 | Loss: 0.3683 | Test Acc: 0.5850\n",
            "[SineKAN HIGGS] Epoch 38 | Loss: 0.3522 | Test Acc: 0.5900\n",
            "[SineKAN HIGGS] Epoch 39 | Loss: 0.3373 | Test Acc: 0.6100\n",
            "[SineKAN HIGGS] Epoch 40 | Loss: 0.3239 | Test Acc: 0.5700\n",
            "[SineKAN HIGGS] Epoch 41 | Loss: 0.3114 | Test Acc: 0.6050\n",
            "[SineKAN HIGGS] Epoch 42 | Loss: 0.2972 | Test Acc: 0.5900\n",
            "[SineKAN HIGGS] Epoch 43 | Loss: 0.2812 | Test Acc: 0.5850\n",
            "[SineKAN HIGGS] Epoch 44 | Loss: 0.2680 | Test Acc: 0.5700\n",
            "[SineKAN HIGGS] Epoch 45 | Loss: 0.2561 | Test Acc: 0.5750\n",
            "[SineKAN HIGGS] Epoch 46 | Loss: 0.2423 | Test Acc: 0.5750\n",
            "[SineKAN HIGGS] Epoch 47 | Loss: 0.2304 | Test Acc: 0.5850\n",
            "[SineKAN HIGGS] Epoch 48 | Loss: 0.2207 | Test Acc: 0.5900\n",
            "[SineKAN HIGGS] Epoch 49 | Loss: 0.2095 | Test Acc: 0.5900\n",
            "[SineKAN HIGGS] Epoch 50 | Loss: 0.1974 | Test Acc: 0.5900\n"
          ]
        }
      ],
      "source": [
        "import pennylane as qml\n",
        "\n",
        "class QSVTLayerMulti(nn.Module):\n",
        "    def __init__(self, coeff_list):\n",
        "        super().__init__()\n",
        "        self.coeff_list = nn.ParameterList([\n",
        "            nn.Parameter(torch.tensor(coeff, dtype=torch.float32), requires_grad=True)\n",
        "            for coeff in coeff_list\n",
        "        ])\n",
        "        self.theta = nn.Parameter(torch.randn(1))\n",
        "        self.dev = qml.device(\"default.qubit\", wires=1)\n",
        "\n",
        "        @qml.qnode(self.dev, interface=\"torch\", diff_method=\"backprop\")\n",
        "        def circuit(x_scalar, theta_val, coeffs_val):\n",
        "            x_scalar = torch.clamp(x_scalar, -1.0, 1.0)\n",
        "            qml.RY(x_scalar, wires=0)\n",
        "            A = x_scalar.detach().cpu().numpy().reshape(1, 1)\n",
        "            coeffs_np = coeffs_val.detach().cpu().numpy()\n",
        "            qml.qsvt(A, coeffs_np, encoding_wires=[0], block_encoding=\"embedding\")\n",
        "            qml.RZ(theta_val[0], wires=0)\n",
        "            return qml.expval(qml.PauliZ(0))\n",
        "\n",
        "        self.circuit = circuit\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, D = x.shape\n",
        "        poly_outputs = []\n",
        "        for coeffs in self.coeff_list:\n",
        "            outputs = []\n",
        "            for i in range(D):\n",
        "                out_i = [self.circuit(x[b, i], self.theta, coeffs) for b in range(B)]\n",
        "                outputs.append(torch.stack(out_i))\n",
        "            poly_outputs.append(torch.stack(outputs, dim=1))  # [B, D]\n",
        "        return torch.stack(poly_outputs, dim=2)  # [B, D, P]\n",
        "\n",
        "def make_quantum_lcu_layer():\n",
        "    dev = qml.device(\"default.qubit\", wires=3)\n",
        "\n",
        "    @qml.qnode(dev, interface=\"torch\", diff_method=\"backprop\")\n",
        "    def lcu_circuit(val, weight):\n",
        "        qml.RY(val, wires=0)\n",
        "        qml.RY(weight, wires=1)\n",
        "        qml.CNOT(wires=[0, 2])\n",
        "        angle = torch.clamp(val * weight, -math.pi, math.pi)\n",
        "        qml.ctrl(qml.RZ, control=0)(angle, wires=2)\n",
        "        return qml.expval(qml.PauliZ(2))\n",
        "\n",
        "    def forward_fn(qsvt_output, weights):  # [B, D, P], [D, P]\n",
        "        B, D, P = qsvt_output.shape\n",
        "        results = []\n",
        "        for b in range(B):\n",
        "            vals = []\n",
        "            for i in range(D):\n",
        "                acc = []\n",
        "                for p in range(P):\n",
        "                    acc.append(lcu_circuit(qsvt_output[b, i, p], weights[i, p]))\n",
        "                vals.append(torch.stack(acc))  # [P]\n",
        "            results.append(torch.stack(vals))  # [D, P]\n",
        "        return torch.stack(results)  # [B, D, P]\n",
        "\n",
        "    return forward_fn\n",
        "\n",
        "def make_quantum_sum_layer():\n",
        "    dev = qml.device(\"default.qubit\", wires=3)\n",
        "\n",
        "    @qml.qnode(dev, interface=\"torch\", diff_method=\"backprop\")\n",
        "    def quantum_sum(val1, val2):\n",
        "        qml.Hadamard(wires=0)\n",
        "        qml.RY(val1, wires=1)\n",
        "        qml.RY(val2, wires=2)\n",
        "        qml.CNOT(wires=[1, 0])\n",
        "        qml.CNOT(wires=[2, 0])\n",
        "        return qml.expval(qml.PauliZ(0))\n",
        "\n",
        "    def forward_fn(lcu_output):  # [B, D, P] where P == 2\n",
        "        B, D, P = lcu_output.shape\n",
        "        assert P == 2, \"Quantum sum layer assumes exactly P=2 polynomials\"\n",
        "        summed = []\n",
        "        for b in range(B):\n",
        "            summed_d = []\n",
        "            for d in range(D):\n",
        "                val1 = lcu_output[b, d, 0]\n",
        "                val2 = lcu_output[b, d, 1]\n",
        "                summed_d.append(quantum_sum(val1, val2))\n",
        "            summed.append(torch.stack(summed_d))\n",
        "        return torch.stack(summed)  # [B, D]\n",
        "\n",
        "    return forward_fn\n",
        "\n",
        "class SineKANLayer(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, grid_size=5):\n",
        "        super().__init__()\n",
        "        self.grid_size = grid_size\n",
        "        self.A, self.K, self.C = 0.9724, 0.9884, 0.9994\n",
        "        self.grid_norm = (torch.arange(grid_size, dtype=torch.float32) + 1).reshape(1, 1, grid_size)\n",
        "        amp = torch.empty(output_dim, input_dim, 1).normal_(0, .4) / output_dim / self.grid_norm\n",
        "        self.amplitudes = nn.Parameter(amp.float())\n",
        "        self.freq = nn.Parameter(torch.arange(1, grid_size + 1).float().reshape(1, 1, 1, grid_size))\n",
        "        grid_phase = torch.arange(1, grid_size + 1, dtype=torch.float32).reshape(1, 1, 1, grid_size) / (grid_size + 1)\n",
        "        input_phase = torch.linspace(0, math.pi, input_dim, dtype=torch.float32).reshape(1, 1, input_dim, 1)\n",
        "        phase = grid_phase + input_phase\n",
        "        for _ in range(1, grid_size):\n",
        "            phase = (self.A * grid_size ** (-self.K) + self.C) * phase\n",
        "        self.register_buffer(\"phase\", phase)\n",
        "        self.bias = nn.Parameter(torch.ones(1, output_dim).float() / output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.to(self.amplitudes.dtype)\n",
        "        x = x.view(x.shape[0], -1).unsqueeze(-1).unsqueeze(1)\n",
        "        s = torch.sin(x * self.freq + self.phase)\n",
        "        y = torch.einsum('bijd,ojd->bo', s, self.amplitudes)\n",
        "        return y + self.bias\n",
        "\n",
        "class SineKAN(nn.Module):\n",
        "    def __init__(self, input_dim, hidden=64, output_dim=2):\n",
        "        super().__init__()\n",
        "        self.l1 = SineKANLayer(input_dim, hidden)\n",
        "        self.l2 = SineKANLayer(hidden, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.l2(self.l1(x))\n",
        "\n",
        "class QSVT_LCU_QuantumSum_SineKAN_Model(nn.Module):\n",
        "    def __init__(self, input_dim, coeff_list):\n",
        "        super().__init__()\n",
        "        self.qsvt = QSVTLayerMulti(coeff_list)\n",
        "        self.lcu_weights = nn.Parameter(torch.randn(input_dim, len(coeff_list)))\n",
        "        self.quantum_lcu = make_quantum_lcu_layer()\n",
        "        self.quantum_sum = make_quantum_sum_layer()\n",
        "        self.kan = SineKAN(input_dim=input_dim, hidden=64, output_dim=2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        qsvt_out = self.qsvt(x)                            # [B, D, P]\n",
        "        lcu_out = self.quantum_lcu(qsvt_out, self.lcu_weights)  # [B, D, P]\n",
        "        quantum_summed = self.quantum_sum(lcu_out)              # [B, D]\n",
        "        return self.kan(quantum_summed)\n",
        "\n",
        "\n",
        "def load_higgs_data(path, Nsamples=1000):\n",
        "    df = pd.read_csv(path, header=None)\n",
        "    df = df.sample(frac=1.0, random_state=42)\n",
        "    X = df.iloc[:Nsamples, 1:].values\n",
        "    y = df.iloc[:Nsamples, 0].values\n",
        "    X_min = X.min(axis=1, keepdims=True)\n",
        "    X_max = X.max(axis=1, keepdims=True)\n",
        "    X_scaled = 2 * (X - X_min) / (X_max - X_min + 1e-8) - 1\n",
        "    return train_test_split(\n",
        "        torch.tensor(X_scaled, dtype=torch.float32),\n",
        "        torch.tensor(y, dtype=torch.long),\n",
        "        test_size=0.2,\n",
        "        stratify=y,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "def train_model(file_path):\n",
        "    X_train, X_test, y_train, y_test = load_higgs_data(file_path, Nsamples=1000)\n",
        "    input_dim = X_train.shape[1]\n",
        "\n",
        "    coeff_list = [\n",
        "        [0.0, 1.0, 0.0, -0.5],      \n",
        "        [0.0, 0.8, 0.0, 0.3]         \n",
        "    ]\n",
        "\n",
        "    model = QSVT_LCU_QuantumSum_SineKAN_Model(input_dim, coeff_list)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    X_train, X_test = X_train.to(device), X_test.to(device)\n",
        "    y_train, y_test = y_train.to(device), y_test.to(device)\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=1e-2)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    for epoch in range(30):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        preds = model(X_train)\n",
        "        loss = criterion(preds, y_train)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            acc = (model(X_test).argmax(dim=1) == y_test).float().mean().item()\n",
        "        print(f\"[QSVT+LCU+Sum+SineKAN] Epoch {epoch+1:02d} | Loss: {loss.item():.4f} | Test Acc: {acc:.4f}\")\n",
        "\n",
        "train_sinekan_higgs(file_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FrXK-UvfvmXN",
        "outputId": "65033670-5d64-41df-e81f-7e99968d40fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[MLP] Epoch 01 | Loss: 0.6973 | Test Acc: 0.4700\n",
            "[MLP] Epoch 02 | Loss: 0.7248 | Test Acc: 0.5200\n",
            "[MLP] Epoch 03 | Loss: 0.6844 | Test Acc: 0.5250\n",
            "[MLP] Epoch 04 | Loss: 0.6873 | Test Acc: 0.5250\n",
            "[MLP] Epoch 05 | Loss: 0.6812 | Test Acc: 0.5400\n",
            "[MLP] Epoch 06 | Loss: 0.6766 | Test Acc: 0.5300\n",
            "[MLP] Epoch 07 | Loss: 0.6715 | Test Acc: 0.5100\n",
            "[MLP] Epoch 08 | Loss: 0.6633 | Test Acc: 0.4950\n",
            "[MLP] Epoch 09 | Loss: 0.6616 | Test Acc: 0.5100\n",
            "[MLP] Epoch 10 | Loss: 0.6495 | Test Acc: 0.5050\n",
            "[MLP] Epoch 11 | Loss: 0.6340 | Test Acc: 0.5200\n",
            "[MLP] Epoch 12 | Loss: 0.6225 | Test Acc: 0.5350\n",
            "[MLP] Epoch 13 | Loss: 0.6029 | Test Acc: 0.5700\n",
            "[MLP] Epoch 14 | Loss: 0.5945 | Test Acc: 0.5400\n",
            "[MLP] Epoch 15 | Loss: 0.5667 | Test Acc: 0.5750\n",
            "[MLP] Epoch 16 | Loss: 0.5599 | Test Acc: 0.5950\n",
            "[MLP] Epoch 17 | Loss: 0.5421 | Test Acc: 0.5650\n",
            "[MLP] Epoch 18 | Loss: 0.5434 | Test Acc: 0.5600\n",
            "[MLP] Epoch 19 | Loss: 0.5300 | Test Acc: 0.5600\n",
            "[MLP] Epoch 20 | Loss: 0.4948 | Test Acc: 0.5200\n",
            "[MLP] Epoch 21 | Loss: 0.4889 | Test Acc: 0.5450\n",
            "[MLP] Epoch 22 | Loss: 0.4383 | Test Acc: 0.5800\n",
            "[MLP] Epoch 23 | Loss: 0.4494 | Test Acc: 0.5550\n",
            "[MLP] Epoch 24 | Loss: 0.3987 | Test Acc: 0.5550\n",
            "[MLP] Epoch 25 | Loss: 0.4147 | Test Acc: 0.5900\n",
            "[MLP] Epoch 26 | Loss: 0.4129 | Test Acc: 0.5900\n",
            "[MLP] Epoch 27 | Loss: 0.4065 | Test Acc: 0.6150\n",
            "[MLP] Epoch 28 | Loss: 0.3643 | Test Acc: 0.6150\n",
            "[MLP] Epoch 29 | Loss: 0.3431 | Test Acc: 0.6150\n",
            "[MLP] Epoch 30 | Loss: 0.3459 | Test Acc: 0.6200\n",
            "[MLP] Epoch 31 | Loss: 0.3414 | Test Acc: 0.6250\n",
            "[MLP] Epoch 32 | Loss: 0.3037 | Test Acc: 0.6350\n",
            "[MLP] Epoch 33 | Loss: 0.3066 | Test Acc: 0.6200\n",
            "[MLP] Epoch 34 | Loss: 0.3170 | Test Acc: 0.6350\n",
            "[MLP] Epoch 35 | Loss: 0.2724 | Test Acc: 0.6200\n",
            "[MLP] Epoch 36 | Loss: 0.2795 | Test Acc: 0.6200\n",
            "[MLP] Epoch 37 | Loss: 0.2413 | Test Acc: 0.6150\n",
            "[MLP] Epoch 38 | Loss: 0.2370 | Test Acc: 0.6250\n",
            "[MLP] Epoch 39 | Loss: 0.2495 | Test Acc: 0.5750\n",
            "[MLP] Epoch 40 | Loss: 0.2513 | Test Acc: 0.6200\n",
            "[MLP] Epoch 41 | Loss: 0.2113 | Test Acc: 0.5850\n",
            "[MLP] Epoch 42 | Loss: 0.2150 | Test Acc: 0.5900\n",
            "[MLP] Epoch 43 | Loss: 0.2081 | Test Acc: 0.6000\n",
            "[MLP] Epoch 44 | Loss: 0.1909 | Test Acc: 0.6100\n",
            "[MLP] Epoch 45 | Loss: 0.1841 | Test Acc: 0.5950\n",
            "[MLP] Epoch 46 | Loss: 0.1864 | Test Acc: 0.5650\n",
            "[MLP] Epoch 47 | Loss: 0.1813 | Test Acc: 0.6150\n",
            "[MLP] Epoch 48 | Loss: 0.1704 | Test Acc: 0.6000\n",
            "[MLP] Epoch 49 | Loss: 0.1632 | Test Acc: 0.6050\n",
            "[MLP] Epoch 50 | Loss: 0.1578 | Test Acc: 0.6400\n",
            "[MLP] Epoch 51 | Loss: 0.1486 | Test Acc: 0.6250\n",
            "[MLP] Epoch 52 | Loss: 0.1229 | Test Acc: 0.6250\n",
            "[MLP] Epoch 53 | Loss: 0.1555 | Test Acc: 0.6250\n",
            "[MLP] Epoch 54 | Loss: 0.1414 | Test Acc: 0.6000\n",
            "[MLP] Epoch 55 | Loss: 0.1381 | Test Acc: 0.6250\n",
            "[MLP] Epoch 56 | Loss: 0.1218 | Test Acc: 0.6200\n",
            "[MLP] Epoch 57 | Loss: 0.1432 | Test Acc: 0.6150\n",
            "[MLP] Epoch 58 | Loss: 0.1476 | Test Acc: 0.6200\n",
            "[MLP] Epoch 59 | Loss: 0.1235 | Test Acc: 0.6250\n",
            "[MLP] Epoch 60 | Loss: 0.1280 | Test Acc: 0.6200\n",
            "[MLP] Epoch 61 | Loss: 0.1227 | Test Acc: 0.6200\n",
            "[MLP] Epoch 62 | Loss: 0.1023 | Test Acc: 0.6150\n",
            "[MLP] Epoch 63 | Loss: 0.0995 | Test Acc: 0.6050\n",
            "[MLP] Epoch 64 | Loss: 0.1144 | Test Acc: 0.6250\n",
            "[MLP] Epoch 65 | Loss: 0.1020 | Test Acc: 0.6200\n",
            "[MLP] Epoch 66 | Loss: 0.1323 | Test Acc: 0.6300\n",
            "[MLP] Epoch 67 | Loss: 0.1305 | Test Acc: 0.6350\n",
            "[MLP] Epoch 68 | Loss: 0.1016 | Test Acc: 0.6450\n",
            "[MLP] Epoch 69 | Loss: 0.1178 | Test Acc: 0.6550\n",
            "[MLP] Epoch 70 | Loss: 0.1135 | Test Acc: 0.6050\n",
            "[MLP] Epoch 71 | Loss: 0.1194 | Test Acc: 0.6000\n",
            "[MLP] Epoch 72 | Loss: 0.1225 | Test Acc: 0.6200\n",
            "[MLP] Epoch 73 | Loss: 0.1147 | Test Acc: 0.6200\n",
            "[MLP] Epoch 74 | Loss: 0.1042 | Test Acc: 0.6250\n",
            "[MLP] Epoch 75 | Loss: 0.1193 | Test Acc: 0.6100\n",
            "[MLP] Epoch 76 | Loss: 0.1183 | Test Acc: 0.6200\n",
            "[MLP] Epoch 77 | Loss: 0.0963 | Test Acc: 0.6350\n",
            "[MLP] Epoch 78 | Loss: 0.0980 | Test Acc: 0.6150\n",
            "[MLP] Epoch 79 | Loss: 0.0903 | Test Acc: 0.6050\n",
            "[MLP] Epoch 80 | Loss: 0.1072 | Test Acc: 0.6100\n",
            "[MLP] Epoch 81 | Loss: 0.0897 | Test Acc: 0.6100\n",
            "[MLP] Epoch 82 | Loss: 0.0899 | Test Acc: 0.6150\n",
            "[MLP] Epoch 83 | Loss: 0.0781 | Test Acc: 0.6150\n",
            "[MLP] Epoch 84 | Loss: 0.0890 | Test Acc: 0.6000\n",
            "[MLP] Epoch 85 | Loss: 0.0825 | Test Acc: 0.6150\n",
            "[MLP] Epoch 86 | Loss: 0.0683 | Test Acc: 0.6250\n",
            "[MLP] Epoch 87 | Loss: 0.0873 | Test Acc: 0.6150\n",
            "[MLP] Epoch 88 | Loss: 0.0765 | Test Acc: 0.5850\n",
            "[MLP] Epoch 89 | Loss: 0.0741 | Test Acc: 0.5900\n",
            "[MLP] Epoch 90 | Loss: 0.0891 | Test Acc: 0.5900\n",
            "[MLP] Epoch 91 | Loss: 0.0951 | Test Acc: 0.5800\n",
            "[MLP] Epoch 92 | Loss: 0.0636 | Test Acc: 0.5750\n",
            "[MLP] Epoch 93 | Loss: 0.0837 | Test Acc: 0.5950\n",
            "[MLP] Epoch 94 | Loss: 0.0979 | Test Acc: 0.5950\n",
            "[MLP] Epoch 95 | Loss: 0.0583 | Test Acc: 0.6200\n",
            "[MLP] Epoch 96 | Loss: 0.0710 | Test Acc: 0.6100\n",
            "[MLP] Epoch 97 | Loss: 0.0817 | Test Acc: 0.6150\n",
            "[MLP] Epoch 98 | Loss: 0.0771 | Test Acc: 0.6200\n",
            "[MLP] Epoch 99 | Loss: 0.0680 | Test Acc: 0.6050\n",
            "[MLP] Epoch 100 | Loss: 0.0691 | Test Acc: 0.6000\n"
          ]
        }
      ],
      "source": [
        "class MLPModel(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim=2):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.LayerNorm(128),\n",
        "            nn.Dropout(0.2),\n",
        "\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.LayerNorm(64),\n",
        "            nn.Dropout(0.2),\n",
        "\n",
        "            nn.Linear(64, 32),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.Linear(32, output_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "def load_higgs_data(path, Nsamples=1000):\n",
        "    df = pd.read_csv(path, header=None)\n",
        "    df = df.sample(frac=1.0, random_state=42)\n",
        "    X = df.iloc[:Nsamples, 1:].values\n",
        "    y = df.iloc[:Nsamples, 0].values\n",
        "    X_min = X.min(axis=1, keepdims=True)\n",
        "    X_max = X.max(axis=1, keepdims=True)\n",
        "    X_scaled = 2 * (X - X_min) / (X_max - X_min + 1e-8) - 1\n",
        "    return train_test_split(\n",
        "        torch.tensor(X_scaled, dtype=torch.float32),\n",
        "        torch.tensor(y, dtype=torch.long),\n",
        "        test_size=0.2,\n",
        "        stratify=y,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "\n",
        "def train_mlp_model(file_path):\n",
        "    X_train, X_test, y_train, y_test = load_higgs_data(file_path, Nsamples=1000)\n",
        "    input_dim = X_train.shape[1]\n",
        "\n",
        "    model = MLPModel(input_dim=input_dim, output_dim=2)\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    X_train, X_test = X_train.to(device), X_test.to(device)\n",
        "    y_train, y_test = y_train.to(device), y_test.to(device)\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=1e-2)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    for epoch in range(100):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        preds = model(X_train)\n",
        "        loss = criterion(preds, y_train)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            acc = (model(X_test).argmax(dim=1) == y_test).float().mean().item()\n",
        "        print(f\"[MLP] Epoch {epoch+1:02d} | Loss: {loss.item():.4f} | Test Acc: {acc:.4f}\")\n",
        "\n",
        "train_mlp_model(file_path)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
